{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dynamic_selection_methods.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/lit/lit_lit/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__NsPMJF6Gli","executionInfo":{"status":"ok","timestamp":1646172953473,"user_tz":300,"elapsed":987,"user":{"displayName":"Enoch Samuel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16090837763996882893"}},"outputId":"a0564d56-71cc-4312-d9d3-caaaceab8aa3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import deslib\n","from deslib.dcs import OLA, MLA\n","from deslib.des import METADES, KNORAU, DESMI, DESP\n","from deslib.des.knora_e import KNORAE\n","from deslib.static import Oracle\n","from sklearn.metrics import accuracy_score\n","import glob\n","import numpy as np\n","from numpy import mean\n","import pickle\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","dict_final_results = {}\n","dict_standard_deviation = {}\n","for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","  print(files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3htYzIj5luF","executionInfo":{"status":"ok","timestamp":1646172953476,"user_tz":300,"elapsed":18,"user":{"displayName":"Enoch Samuel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16090837763996882893"}},"outputId":"1cadf03b-2920-44b5-ea57-e996e8fd7a00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/Adult_dir\n"]}]},{"cell_type":"code","source":["# import deslib\n","# from deslib.dcs import OLA, MLA\n","# from deslib.des import METADES, KNORAU, DESMI, DESP\n","# from deslib.des.knora_e import KNORAE\n","# from deslib.static import Oracle\n","# from sklearn.metrics import accuracy_score\n","# import glob\n","# import numpy as np\n","# from numpy import mean\n","# import pickle\n","# from glob import glob\n","# from sklearn.metrics import roc_auc_score\n","# from sklearn.metrics import auc\n","# import sklearn.metrics as metrics\n","# from sklearn import metrics\n","# from sklearn.metrics import fbeta_score, make_scorer\n","# import sklearn\n","# from numpy import mean\n","\n","\n","# dict_final_results = {}\n","# dict_standard_deviation = {}\n","# for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","#     print(files)\n","#     standard_deviation = []\n","#     knorae_results = []\n","#     accuracy = []\n","#     for items in glob(\"{}/*.*\".format(files)):\n","#         print(items)\n","#         data = np.load(items, allow_pickle=True)\n","#         input_list = data.tolist()  # How to get x_train\n","#         X_train = input_list['FrameStack'][0]\n","#         X_test = input_list['FrameStack'][1]\n","#         y_train = input_list['FrameStack'][2]\n","#         y_test = input_list['FrameStack'][3]\n","#         X_dsel = input_list['FrameStack'][4]\n","#         y_dsel = input_list['FrameStack'][5]\n","\n","#         ###file_name = files.split(\"\\\\\")[1]\n","#         # print(file_name)\n","#         ###item_name = items.split(\"\\\\\")[2].split(\".\")[0].split(\"_\")[2]\n","#         # print(item_name)\n","#         ###ss = file_name.split(\"_\")[0]\n","#         # print(ss)\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_perceptron.pkl\".format(file_name, item_name))[0]\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_perceptron.pkl\".format(file_name, item_name))[0]\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/random_forest.pkl\".format(file_name, item_name))[0]\n","#         # model_load = glob.glob(\"Models_FLT_f/{}/{}_{}.pkl\".format(file_name,ss, item_name))[0]\n","#         # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","#         model_load = glob(\"/content/drive/MyDrive/lit/lit_lit/input_space_model0.pkl\")   #input_space_model0.pkl\n","\n","#         x_merge = np.append(X_train, X_dsel, axis=0)\n","#         y_merge = np.append(y_train, y_dsel, axis=0)\n","\n","#         pool_classifiers = model_load\n","#         # method = KNORAE\n","#         # method = MLA\n","#         # method = OLA\n","#         # method = METADES\n","#         # method = KNORAU\n","#         method = DESMI\n","#         # method = DESP\n","\n","#         ensemble = method(pool_classifiers)\n","#         # ensemble.fit(x_train, y_train)\n","#         ensemble.fit(x_merge, y_merge)\n","#         # Predict new examples:\n","#         yh = ensemble.predict(X_test)\n","#         acc = accuracy_score(y_test, yh)\n","#         print(acc)\n","#         knorae_results.append(acc)\n","\n","#     dict_final_results[\"{}\".format(files)] = '{:.4f}'.format(mean(knorae_results))\n","#     print(dict_final_results)\n","#     np.std(knorae_results, dtype=np.float64)\n","#     standard_deviation.append(np.std(knorae_results, dtype=np.float64))\n","#     dict_standard_deviation[\"{}\".format(files)] = '{:.4f}'.format(mean(standard_deviation))\n","\n","# print(dict_final_results.values())\n","# print(dict_standard_deviation.values())"],"metadata":{"id":"_yWEv2Z9-UTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import uuid\n","import numpy as np\n","import tensorflow as tf\n","import six.moves.cPickle as pickle\n","from six import add_metaclass\n","from abc import ABCMeta, abstractmethod, abstractproperty\n","# from utils import *\n","\n","\"\"\"\n","Object-oriented class for handling neural networks implemented\n","in Tensorflow.\n","\"\"\"\n","@add_metaclass(ABCMeta)\n","class NeuralNetwork():\n","  def __init__(self, name=None, dtype=tf.float32, **kwargs):\n","    self.vals = None # Holds the trained weights of the network\n","    self.name = (name or str(uuid.uuid4())) # Tensorflow variable scope\n","    self.dtype = dtype\n","    self.setup_model(**kwargs)\n","    assert(hasattr(self, 'X'))\n","    assert(hasattr(self, 'y'))\n","    assert(hasattr(self, 'logits'))\n","\n","  def setup_model(self, X=None, y=None, **kw):\n","    \"\"\"Defines common placeholders, then calls rebuild_model\"\"\"\n","    with tf.name_scope(self.name):\n","\n","      self.X = tf.compat.v1.placeholder(self.dtype, self.x_shape , name=\"X\") if X is None else X\n","      self.y = tf.compat.v1.placeholder(self.dtype, self.y_shape, name=\"y\") if y is None else y\n","      self.sample_weight = tf.compat.v1.placeholder(self.dtype, [None], name=\"sample_weight\")\n","      self.is_train = tf.compat.v1.placeholder_with_default(\n","          tf.constant(False, dtype=tf.bool), shape=(), name=\"is_train\")\n","    self.model = self.rebuild_model(self.X, **kw)\n","    self.recompute_vars()\n","\n","  def rebuild_model(self, X, reuse=None, **kw):\n","    \"\"\"Override this in subclasses. Define Tensorflow operations and return\n","    list whose last entry is logits.\"\"\"\n","\n","  @property\n","  def logits(self):\n","    return self.model[-1]\n","\n","  @abstractproperty\n","  def x_shape(self):\n","    \"\"\"Specify the shape of X; for MNIST, this could be [None, 784]\"\"\"\n","\n","  @abstractproperty\n","  def y_shape(self):\n","    \"\"\"Specify the shape of y; for MNIST, this would be [None, 10]\"\"\"\n","\n","  @property\n","  def num_features(self):\n","    return np.product(self.x_shape[1:])\n","\n","  @property\n","  def num_classes(self):\n","      \n","    return np.product(self.y_shape[1:])\n","\n","  @property\n","  def trainable_vars(self):\n","    \"\"\"Return this model's trainable variables\"\"\"\n","    return [v for v in tf.compat.v1.trainable_variables() if v in self.vars]\n","\n","  def input_grad(self, f):\n","    \"\"\"Helper to take input gradients\"\"\"\n","    return tf.gradients(f, self.X)[0]\n","\n","  def cross_entropy_with(self, y):\n","    \"\"\"Compute sample-weighted cross-entropy classification loss\"\"\"\n","    w = self.sample_weight / tf.reduce_sum(self.sample_weight)\n","    return tf.reduce_sum(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=y) * w)\n","\n","  @cachedproperty\n","  def preds(self):\n","    \"\"\"Tensorflow operation to return predicted labels.\"\"\"\n","    return tf.argmax(self.logits, axis=1)\n","\n","  @cachedproperty\n","  def probs(self):\n","    \"\"\"Tensorflow operation to return predicted label probabilities.\"\"\"\n","    return tf.nn.softmax(self.logits)\n","\n","  @cachedproperty\n","  def logps(self):\n","    \"\"\"Tensorflow operation to return predicted label log-probabilities.\"\"\"\n","    return self.logits - tf.reduce_logsumexp(self.logits, 1, keepdims=True)\n","\n","  @cachedproperty\n","  def grad_sum_logps(self):\n","    \"\"\"Tensorflow operation returning gradient of the sum of log-probabilities.\n","    Can be used as the gradient for LIT for multi-class classification (doesn't\n","    require labels).\"\"\"\n","    return self.input_grad(self.logps)\n","\n","  @cachedproperty\n","  def l2_weights(self):\n","    \"\"\"Tensorflow operation returning sum of squared weight values\"\"\"\n","    return tf.add_n([l2_loss(v) for v in self.trainable_vars])\n","\n","  @cachedproperty\n","  def cross_entropy(self):\n","    \"\"\"Tensorflow operation returning classification cross-entropy\"\"\"\n","    return self.cross_entropy_with(self.y)\n","\n","  @cachedproperty\n","  def cross_entropy_input_gradients(self):\n","    \"\"\"Tensorflow operation returning gradient of the loss. Can be used as the\n","    gradient for LIT for multi-class classification but does require labels.\"\"\"\n","    return self.input_grad(self.cross_entropy)\n","\n","  @cachedproperty\n","  def predicted_logit_input_gradients(self):\n","    \"\"\"Tensorflow operation returning gradient of the predicted log-odds.\n","    Mostly useful for visualization rather than training.\"\"\"\n","    return self.input_grad(self.logits * self.y)\n","\n","  @cachedproperty\n","  def binary_logits(self):\n","    \"\"\"Tensorflow operation returning the actual predicted log-odds (binary\n","    only).\"\"\"\n","    assert(self.num_classes == 2)\n","    return self.logps[:,1] - self.logps[:,0]\n","\n","  @cachedproperty\n","  def binary_logit_input_gradients(self):\n","    \"\"\"Tensorflow operation returning gradient of the predicted binary\n","    log-odds. This is what we use for LIT in binary classification.\"\"\"\n","    return self.input_grad(self.binary_logits)\n","\n","  @cachedproperty\n","  def accuracy(self):\n","    \"\"\"Tensorflow operation returning classification accuracy.\"\"\"\n","    return tf.reduce_mean(tf.cast(tf.equal(self.preds, tf.argmax(self.y, 1)), dtype=tf.float32))\n","\n","  def score(self, X, y, **kw):\n","    \"\"\"Compute classification accuracy for numpy inputs and labels.\"\"\"\n","    if len(y.shape) == 2:\n","      return np.mean(self.predict(X, **kw) == np.argmax(y, 1))\n","    else:\n","      return np.mean(self.predict(X, **kw) == y)\n","\n","  def predict(self, X, **kw):\n","    \"\"\"Compute predictions for numpy inputs.\"\"\"\n","    with tf.Session() as sess:\n","      self.init(sess)\n","      return self.batch_eval(sess, self.preds, X, **kw)\n","\n","  def predict_logits(self, X, **kw):\n","    \"\"\"Compute raw logits for numpy inputs.\"\"\"\n","    with tf.compat.v1.Session() as sess:\n","      self.init(sess)\n","      return self.batch_eval(sess, self.logits, X, **kw)\n","\n","  def predict_binary_logodds(self, X, **kw):\n","    \"\"\"Compute predicted binary log-odds for numpy inputs.\"\"\"\n","    with tf.compat.v1.Session() as sess:\n","      self.init(sess)\n","      return self.batch_eval(sess, self.binary_logits, X, **kw)\n","\n","  def predict_proba(self, X, **kw):\n","    \"\"\"Compute predicted probabilities for numpy inputs.\"\"\"\n","    with tf.compat.v1.Session() as sess:\n","      self.init(sess)\n","      return self.batch_eval(sess, self.probs, X, **kw)\n","\n","  def batch_eval(self, sess, quantity, X, n=256):\n","    \"\"\"Internal helper to batch computations (prevents memory issues)\"\"\"\n","    vals = sess.run(quantity, feed_dict={ self.X: X[:n] })\n","    stack = np.vstack if len(vals.shape) > 1 else np.hstack\n","    for i in range(n, len(X), n):\n","      vals = stack((vals, sess.run(quantity, feed_dict={ self.X: X[i:i+n] })))\n","    return vals\n","\n","  def input_gradients(self, X, y=None, n=256, **kw):\n","    \"\"\"Computes different kinds of input gradients for inputs (and optionally\n","    labels). See input_gradients_ for details.\"\"\"\n","    with tf.compat.v1.Session() as sess:\n","      self.init(sess)\n","      return self.batch_input_gradients_(sess, X, y, n, **kw)\n","\n","  def batch_input_gradients_(self, sess, X, y=None, n=256, **kw):\n","    yy = y[:n] if y is not None and not isint(y) else y\n","    grads = self.input_gradients_(sess, X[:n], yy, **kw)\n","    for i in range(n, len(X), n):\n","      yy = y[i:i+n] if y is not None and not isint(y) else y\n","      grads = np.vstack((grads,\n","        self.input_gradients_(sess, X[i:i+n], yy, **kw)))\n","    return grads\n","\n","  def input_gradients_(self, sess, X, y=None, logits=False, quantity=None):\n","    if quantity is not None:\n","      return sess.run(quantity, feed_dict={ self.X: X })\n","    if y is None:\n","      return sess.run(self.grad_sum_logps, feed_dict={ self.X: X })\n","    elif logits and self.num_classes == 2:\n","      return sess.run(self.binary_logit_input_gradients, feed_dict={ self.X: X })\n","    elif isint(y):\n","      y = onehot(np.array([y]*len(X)), self.num_classes)\n","    feed = { self.X: X, self.y: y, self.sample_weight: np.ones(len(X)) }\n","    if logits:\n","      return sess.run(self.predicted_logit_input_gradients, feed_dict=feed)\n","    else:\n","      return sess.run(self.cross_entropy_input_gradients, feed_dict=feed)\n","\n","  def loss_function(self, l2_weights=0., **kw):\n","    \"\"\"Construct the loss function Tensorflow op given hyperparameters.\"\"\"\n","    log_likelihood = self.cross_entropy\n","    if l2_weights > 0:\n","      log_prior = l2_weights * self.l2_weights\n","    else:\n","      log_prior = 0\n","    return log_likelihood + log_prior\n","\n","  def fit(self, X, y, loss_fn=None, init=False, sample_weight=None, **kw):\n","    \"\"\"Fit the neural network on the particular dataset.\"\"\"\n","    if loss_fn is None:\n","      loss_fn = self.loss_function(**kw)\n","    if len(y.shape) == 1:\n","      y = onehot(y, self.num_classes)\n","    if sample_weight is None:\n","      sample_weight = np.ones(len(X))\n","    ops = { 'xent': self.cross_entropy, 'loss': loss_fn, 'accu': self.accuracy }\n","    batches = train_batches([self], X, y, sample_weight=sample_weight, **kw)\n","    with tf.compat.v1.Session() as sess:\n","      if init: self.init(sess)\n","      minimize(sess, loss_fn, batches, ops, **kw)\n","      self.vals = [v.eval() for v in self.vars]\n","\n","  def recompute_vars(self):\n","    \"\"\"Determine which Tensorflow variables are associated with this\n","    network.\"\"\"\n","    self.vars = tf.compat.v1.get_default_graph().get_collection(\n","        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n","\n","  def init(self, sess):\n","    \"\"\"Prepare this network to be used in a Tensorflow session.\"\"\"\n","    if self.vals is None:\n","      sess.run(tf.global_variables_initializer())\n","    else:\n","      for var, val in zip(self.vars, self.vals):\n","        sess.run(var.assign(val))\n","\n","  def save(self, filename):\n","    \"\"\"Save the weights of the network to a pickle file.\"\"\"\n","    with open(filename, 'wb') as f:\n","      pickle.dump(self.vals, f)\n","\n","  def load(self, filename):\n","    \"\"\"Load the weights of the network from a pickle file.\"\"\"\n","    with open(filename, 'rb') as f:\n","      self.vals = pickle.load(f)"],"metadata":{"id":"ZWyEoSBjjTPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function\n","import six\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","def l1_loss(x):\n","  return tf.reduce_sum(tf.abs(x))\n","\n","def l2_loss(x):\n","  return tf.nn.l2_loss(x)\n","\n","class cachedproperty(object):\n","  \"\"\"Simplified https://github.com/pydanny/cached-property\"\"\"\n","  def __init__(self, function):\n","    self.__doc__ = getattr(function, '__doc__')\n","    self.function = function\n","\n","  def __get__(self, instance, klass):\n","    if instance is None: return self\n","    value = instance.__dict__[self.function.__name__] = self.function(instance)\n","    return value\n","\n","def isint(x):\n","  return isinstance(x, (int, np.int32, np.int64))\n","\n","def onehot(Y, K=None):\n","  if K is None:\n","    K = np.unique(Y)\n","  elif isint(K):\n","    K = list(range(K))\n","  data = np.array([[y == k for k in K] for y in Y]).astype(int)\n","  return data\n","\n","def minibatch_indexes(lenX, batch_size=256, num_epochs=50, **kw):\n","  n = int(np.ceil(lenX / batch_size))\n","  for epoch in range(num_epochs):\n","    for batch in range(n):\n","      i = epoch*n + batch\n","      \n","      yield i, epoch, slice((i%n)*batch_size, ((i%n)+1)*batch_size)\n","\n","def train_feed(idx, models, **kw):\n","  \"\"\"Convert a set of models, a set of indexes, and numpy arrays given by the\n","  keyword arguments to a set of feed dictionaries for each model.\"\"\"\n","  feed = {}\n","  for m in models:\n","   \n","    feed[m.is_train] = True\n","    for dictionary in [kw, kw.get('feed_dict', {})]:\n","      for key, val in six.iteritems(dictionary):\n","        attr = getattr(m, key) if isinstance(key, str) and hasattr(m, key) else key\n","        \n","        if type(attr) == type(m.X):\n","          if len(attr.shape) >= 1:\n","            if attr.shape[0] is None:\n","              feed[attr] = val[idx]\n","  \n","  return feed\n","\n","def train_batches(models, X, y, **kw):\n","  for i, epoch, idx in minibatch_indexes(len(X), **kw):\n","    yield i, epoch, train_feed(idx, models, X=X, y=y, **kw)\n","\n","def reinitialize_variables(sess):\n","  \"\"\"Construct a Tensorflow operation to initialize any variables in its graph\n","  which are not already initialized.\"\"\"\n","  uninitialized_vars = []\n","  for var in tf.compat.v1.global_variables():\n","    try:\n","      sess.run(var)\n","    except tf.errors.FailedPreconditionError:\n","      uninitialized_vars.append(var)\n","  return tf.compat.v1.variables_initializer(uninitialized_vars)\n","\n","def minimize(sess, loss_fn, batches, operations={}, learning_rate=0.001, print_every=None, var_list=None, **kw):\n","  \"\"\"Minimize a loss function over the provided batches of data, possibly\n","  printing progress.\"\"\"\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","  train_op = optimizer.minimize(loss_fn, var_list=var_list)\n","  op_keys = sorted(list(operations.keys()))\n","  ops = [train_op] + [operations[k] for k in op_keys]\n","  t = time.time()\n","  sess.run(reinitialize_variables(sess))\n","  for i, epoch, batch in batches:\n","\n","    results = sess.run(ops, feed_dict=batch)\n","    if print_every and i % print_every == 0:\n","      s = 'Batch {}, epoch {}, time {:.1f}s'.format(i, epoch, time.time() - t)\n","      for j,k in enumerate(op_keys, 1):\n","        s += ', {} {:.4f}'.format(k, results[j])\n","      print(s)\n","\n","def tt_split(X, y, test_size=0.2):\n","  return train_test_split(X, y, test_size=test_size, stratify=y)\n","\n","def elemwise_sq_cos_sim(v, w, eps=1e-8):\n","  assert(len(v.shape) == 2)\n","  assert(len(w.shape) == 2)\n","  num = np.sum(v*w, axis=1)**2\n","  den = np.sum(v*v, axis=1) * np.sum(w*w, axis=1)\n","  return num / (den + eps)\n","\n","def yules_q_statistic(e1, e2, y_test):\n","  n = len(y_test)\n","  n00 = len(e1.intersection(e2))\n","  n01 = len(e1.difference(e2))\n","  n10 = len(e2.difference(e1))\n","  n11 = n - len(e1.union(e2))\n","  assert(n00+n01+n10+n11 == n)\n","  numer = n11*n00 - n01*n10\n","  denom = n11*n00 + n01*n10\n","  if numer == 0:\n","    return 0\n","  else:\n","    return numer / float(denom)\n","\n","def disagreement_measure(e1, e2, y_test):\n","  n = len(y_test)\n","  n01 = len(e1.difference(e2))\n","  n10 = len(e2.difference(e1))\n","  return (n01 + n10) / n\n","\n","def scoring_fun(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = y_pred\n","    else:\n","      preds = y_pred[:,1]\n","    \n","    return accuracy_fun(y_pred, y_true)\n","    #return roc_auc_score(y_true, preds)\n","  else:\n","    return accuracy_fun(y_pred, y_true)\n","\n","def accuracy_fun(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = (y_pred > 0.5).astype(int)\n","    else:\n","      preds = np.argmax(y_pred, axis=1)\n","    return np.mean(y_true == preds)\n","  else:\n","    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n","\n","def error_masks(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = (y_pred > 0.5).astype(int)\n","    else:\n","      preds = np.argmax(y_pred, axis=1)\n","    return (preds != y_true).astype(int)\n","  else:\n","    return (np.argmax(y_true, axis=1) != np.argmax(y_pred, axis=1)).astype(int)"],"metadata":{"id":"Cer71c4aLmYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(NeuralNetwork): \n","  @property\n","  def x_shape(self): return [None, 24]\n","  @property\n","  def y_shape(self): return [None, 2]\n","  def rebuild_model(self, X, **_):\n","    L0 = X\n","    L1 = tf.compat.v1.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.softplus)\n","    L2 = tf.compat.v1.layers.dense(L1, 256, name=self.name+'/L2', activation=tf.nn.softplus)\n","    L3 = tf.compat.v1.layers.dense(L2,  2, name=self.name+'/L3', activation=None)\n","    return [L1, L2, L3]"],"metadata":{"id":"jKKt3CRbLYnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7IzmZJx2Oib","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"error","timestamp":1646173234125,"user_tz":300,"elapsed":366,"user":{"displayName":"Enoch Samuel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16090837763996882893"}},"outputId":"bb4e5bd0-c719-4391-8205-6b2fe60d0476"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/Adult_dir\n","/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/Adult_dir/processed_Adult_1.npy\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-116-6d11bc2f6363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# input_space_models = [Net() for _ in range(2)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0minput_space_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_space_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#input_space_model0.pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-108-9e22c450ac08>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dtype, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Tensorflow variable scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-108-9e22c450ac08>\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self, X, y, **kw)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_shape\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3296\u001b[0m   \"\"\"\n\u001b[1;32m   3297\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3298\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   3299\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   3300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."]}],"source":["import deslib\n","from deslib.dcs import OLA, MLA\n","from deslib.des import METADES, KNORAU, DESMI, DESP\n","from deslib.des.knora_e import KNORAE\n","from deslib.static import Oracle\n","from sklearn.metrics import accuracy_score\n","import glob\n","import numpy as np\n","from numpy import mean\n","import pickle\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","\n","dict_final_results = {}\n","dict_standard_deviation = {}\n","for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    standard_deviation = []\n","    knorae_results = []\n","    accuracy = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_dsel = input_list['FrameStack'][4]\n","        y_dsel = input_list['FrameStack'][5]\n","\n","        ###file_name = files.split(\"\\\\\")[1]\n","        # print(file_name)\n","        ###item_name = items.split(\"\\\\\")[2].split(\".\")[0].split(\"_\")[2]\n","        # print(item_name)\n","        ###ss = file_name.split(\"_\")[0]\n","        # print(ss)\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/random_forest.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"Models_FLT_f/{}/{}_{}.pkl\".format(file_name,ss, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        input_space_models = [Net() for _ in range(2)]\n","        for i, model in enumerate(input_space_models):\n","          model.load(\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"+str(i)+\".pkl\")   #input_space_model0.pkl\n","\n","          x_merge = np.append(X_train, X_dsel, axis=0)\n","          y_merge = np.append(y_train, y_dsel, axis=0)\n","\n","          pool_classifiers = model_load\n","          method = KNORAE\n","          # method = MLA\n","          # method = OLA\n","          # method = METADES\n","          # method = KNORAU\n","          # method = DESMI\n","          # method = DESP\n","\n","          ensemble = method(pool_classifiers)\n","          # ensemble.fit(X_train, y_train)\n","          ensemble.fit(x_merge, y_merge)\n","          # Predict new examples:\n","          yh = ensemble.predict(X_test)\n","          acc = accuracy_score(y_test, yh)\n","          print(acc)\n","          knorae_results.append(acc)\n","\n","    dict_final_results[\"{}\".format(files)] = '{:.4f}'.format(mean(knorae_results))\n","    print(dict_final_results)\n","    np.std(knorae_results, dtype=np.float64)\n","    standard_deviation.append(np.std(knorae_results, dtype=np.float64))\n","    dict_standard_deviation[\"{}\".format(files)] = '{:.4f}'.format(mean(standard_deviation))\n","\n","print(dict_final_results.values())\n","print(dict_standard_deviation.values())"]},{"cell_type":"code","source":["# bedone save\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import numpy as np\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","dict_results = {}\n","dict_sd_pc = {}\n","for files in glob('//content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    pc = []\n","    sd_pc = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        item_name = items.split(\".\")[0]\n","        print(item_name)\n","        ss = item_name.split(\"_\")[4] + item_name.split(\"_\")[5]\n","        print(ss)\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_val = input_list['FrameStack'][4]\n","        y_val = input_list['FrameStack'][5]\n","        print(len(X_train))\n","\n","        y_shape = max(y_train)+1\n","        class Net(NeuralNetwork): \n","          @property\n","          def x_shape(self): return [None, X_train.shape[1]]\n","          @property\n","          def y_shape(self): return [None, y_shape]\n","          def rebuild_model(self, X, **_):\n","            L0 = X\n","            L1 = tf.compat.v1.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.softplus)\n","            L2 = tf.compat.v1.layers.dense(L1, 256, name=self.name+'/L2', activation=tf.nn.softplus)\n","            L3 = tf.compat.v1.layers.dense(L2, y_shape , name=self.name+'/L3', activation=None)\n","            return [L1, L2, L3]\n","\n","      \n","        tf.compat.v1.reset_default_graph()\n","\n","        print(\"train restarts\")\n","        random_restart_models = train_restart_models(Net, 2, X_train, y_train, num_epochs=100, print_every=50)\n","\n","        print(\"train input-space LIT\")\n","        input_space_models = train_diverse_models(Net, 2, X_train, y_train, num_epochs=100, print_every=50)\n","        all_model_probs = np.array([ model.predict_proba(X_test)[:,1] for model in input_space_models ])\n","        average_probs = all_model_probs.mean(axis=0) # average predicted probability\n","        variance_probs = all_model_probs.var(axis=0) # variance = measure of uncertainty\n","        label_predictions = (average_probs >= 0.5).astype(int) # threshold average to get label prediction\n","        accuracy = np.mean(label_predictions == y_test)\n","        print(items)\n","        print(accuracy)\n","#         pc.append(accuracy)\n","#         dict_results[\"{}\".format(files)] = '{:.4f}'.format(mean(pc))\n","#         np.std(pc, dtype=np.float64)\n","#         sd_pc.append(np.std(pc, dtype=np.float64))\n","\n","#     dict_sd_pc[\"{}\".format(files)] = '{:.4f}'.format(mean(sd_pc))\n","# print(dict_results)\n","# print(dict_results.values())\n","# print(dict_sd_pc.values())"],"metadata":{"id":"gjjiJQNqC5xw"},"execution_count":null,"outputs":[]}]}