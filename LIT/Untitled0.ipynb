{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyPMZm5VBDWlxUc5WHRSlqDD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZLvCcrX3AzM","executionInfo":{"status":"ok","timestamp":1643230902266,"user_tz":-210,"elapsed":21671,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}},"outputId":"298273c2-8c96-4d57-d8f1-91f36fa032dc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/project6/lit-master/lit-master')"],"metadata":{"id":"oEY8cthzqf74","executionInfo":{"status":"ok","timestamp":1643230902267,"user_tz":-210,"elapsed":6,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import numpy as np\n","import tensorflow as tf\n","from scipy.stats import pearsonr\n","from utils import *\n","from neural_network import *\n","from ensembling_methods import *\n","from glob import glob\n"],"metadata":{"id":"3LSkR2_0yEot","executionInfo":{"status":"ok","timestamp":1643230906626,"user_tz":-210,"elapsed":4363,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--save_dir\", type=str)\n","parser.add_argument(\"--n_models\", type=int)\n","parser.add_argument(\"--dataset\", type=str)\n","parser.add_argument(\"--split\", type=str, default='none')\n","parser.add_argument('-f')\n","FLAGS = parser.parse_args()"],"metadata":{"id":"vQbQAYahyHpU","executionInfo":{"status":"ok","timestamp":1643207614982,"user_tz":-210,"elapsed":38,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","save_dir = FLAGS.save_dir\n","n_models = FLAGS.n_models\n","dataset = FLAGS.dataset\n","split = FLAGS.split\n"],"metadata":{"id":"WsQ2GB8AuWtj","executionInfo":{"status":"ok","timestamp":1643207614983,"user_tz":-210,"elapsed":36,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["for files in glob('/content/drive/MyDrive/Colab Notebooks/project6/lit-master/lit-master/processed_8/processed_8/*'):\n","    # print(files)\n","    pc = []\n","    sd_pc = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        \n","        data = np.load(items, allow_pickle=True)\n","\n","        input_list = data.tolist()  # How to get x_train\n","        x_train = input_list['FrameStack'][0]\n","        x_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        x_dsel = input_list['FrameStack'][4]\n","        y_dsel = input_list['FrameStack'][5]\n","   "],"metadata":{"id":"XrKK4htBuh7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","\n","# Decide how to split it\n","if split == 'norm':\n","  # If splitting by \"norm\", split train and test by distance from origin,\n","  # but subsplit train and validation randomly\n","  norms = np.linalg.norm(X, axis=1)\n","  midpt = np.median(norms)\n","  X_test = X[np.argwhere(norms > midpt)[:,0]]\n","  y_test = y[np.argwhere(norms > midpt)[:,0]]\n","  X_train = X[np.argwhere(norms <= midpt)[:,0]]\n","  y_train = y[np.argwhere(norms <= midpt)[:,0]]\n","  X_test, X_val, y_test, y_val = tt_split(X_test, y_test)\n","else:\n","  # Otherwise, split train/test/val completely randomly\n","  X_train, X_test, y_train, y_test = tt_split(X, y)\n","  X_train, X_val, y_train, y_val = tt_split(X_train, y_train)\n","\n","if split == 'limit':\n","  # If splitting by \"limit\", use a smaller training set\n","  X_train = X_train[:1000]\n","  y_train = y_train[:1000]\n","'''\n"],"metadata":{"id":"GN6wjpB5ucNX","executionInfo":{"status":"aborted","timestamp":1643207799078,"user_tz":-210,"elapsed":25,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define helpers for printing performance metrics to a csv.\n","cols = (\n","    ['ensemble_type', 'ensem_val_auc']\n","    + ['indiv_auc_avg', 'indiv_auc_std', 'ensem_auc', 'indiv_auc_max', 'indiv_auc_min']\n","    + ['indiv_acc_avg', 'indiv_acc_std', 'ensem_acc', 'indiv_acc_max', 'indiv_acc_min']\n","    + ['q_stat', 'interrater', 'err_corr', 'grad_cos2']\n",")\n","\n","def write_row(row, mode='a+'):\n","  csv = open(save_dir + 'auc_results.csv', mode)\n","  csv.write(','.join(row) + '\\n')\n","  csv.close()\n","\n","write_row(cols, mode='w')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"p-KWdMfmvkBY","executionInfo":{"status":"error","timestamp":1643207834480,"user_tz":-210,"elapsed":538,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}},"outputId":"b3142cfd-a315-44d4-ff11-6e55fa25d6c5"},"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-816f649957f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mwrite_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-816f649957f4>\u001b[0m in \u001b[0;36mwrite_row\u001b[0;34m(row, mode)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'auc_results.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKdFrrNPSgqW","executionInfo":{"status":"aborted","timestamp":1643207799087,"user_tz":-210,"elapsed":32,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"outputs":[],"source":["\n","\n","# Define main helper for evaluating models and saving results\n","def save_models(models, name, moe_auc=None, moe_val=None, moe_acc=None):\n","  row = {'ensemble_type': name}\n","  print(name)\n","  test_preds = []\n","  val_preds = []\n","  accs = []\n","  aucs = []\n","  grads = []\n","\n","  # For each model, save its parameters, compute its individual AUC, and\n","  # compile its predictions\n","  for i, m in enumerate(models):\n","    m.save('{}{}_model{}.pkl'.format(save_dir, name, i))\n","    testp = m.predict_proba(X_test)\n","    valp = m.predict_proba(X_val)\n","    auc = scoring_fun(testp, y_test)\n","    acc = accuracy_fun(testp, y_test)\n","    print('  Model #{} AUC: {:.4f}, acc: {:.4f}'.format(i+1,auc,acc))\n","    aucs.append(auc)\n","    accs.append(acc)\n","    test_preds.append(testp)\n","    val_preds.append(valp)\n","    grads.append(m.input_gradients(X_test, logits=(y_test.max() == 1)))\n","\n","  # Save max, min, mean, and standard deviation of individual model AUC\n","  print('  Indiv AUC max: {:.4f}'.format(np.max(aucs)))\n","  print('  Indiv AUC min: {:.4f}'.format(np.min(aucs)))\n","  print('  Indiv AUC mu: {:.4f}'.format(np.mean(aucs)))\n","  print('  Indiv AUC sd: {:.4f}'.format(np.std(aucs)))\n","\n","  row['indiv_auc_max'] = '{:.6f}'.format(np.max(aucs))\n","  row['indiv_auc_min'] = '{:.6f}'.format(np.min(aucs))\n","  row['indiv_auc_avg'] = '{:.6f}'.format(np.mean(aucs))\n","  row['indiv_auc_std'] = '{:.6f}'.format(np.std(aucs))\n","\n","  row['indiv_acc_max'] = '{:.6f}'.format(np.max(accs))\n","  row['indiv_acc_min'] = '{:.6f}'.format(np.min(accs))\n","  row['indiv_acc_avg'] = '{:.6f}'.format(np.mean(accs))\n","  row['indiv_acc_std'] = '{:.6f}'.format(np.std(accs))\n","\n","  # Compute AUC of average prediction\n","  val_preds = np.array(val_preds)\n","  test_preds = np.array(test_preds)\n","  grads = np.array(grads)\n","  avg_auc = scoring_fun(test_preds.mean(axis=0), y_test)\n","  avg_acc = accuracy_fun(test_preds.mean(axis=0), y_test)\n","  print('  Ens. Avg AUC: {:.4f}, acc: {:.4f}'.format(avg_auc, avg_acc))\n","\n","  avg_auc_val = scoring_fun(val_preds.mean(axis=0), y_val)\n","\n","  # Report it (unless it's adaboost and we've been passed its weighted\n","  # predictions)\n","  if moe_auc is None:\n","    row['ensem_auc'] = '{:.6f}'.format(avg_auc)\n","    row['ensem_acc'] = '{:.6f}'.format(avg_acc)\n","    row['ensem_val_auc'] = '{:.6f}'.format(avg_auc_val)\n","  else:\n","    row['ensem_auc'] = '{:.6f}'.format(moe_auc)\n","    row['ensem_acc'] = '{:.6f}'.format(moe_acc)\n","    row['ensem_val_auc'] = '{:.6f}'.format(moe_val)\n","\n","  # If the ensemble had more than one model (i.e. if it wasn't AdaBoost\n","  # terminating early), then compute standard diversity measures (+ ours).\n","  if len(models) > 1:\n","    # First determine where each model erred\n","    ens_errors = [error_masks(preds, y_test) for preds in test_preds]\n","    ens_errsets = [set(np.argwhere(error)[:,0]) for error in ens_errors]\n","\n","    # Compute the error correlation rho_avg\n","    error_corr = np.mean([\n","      pearsonr(err1, err2)[0]\n","      for i,err1 in enumerate(ens_errors)\n","      for err2 in ens_errors[i+1:]])\n","\n","    # Compute the q-statistic\n","    q_stat = np.mean([\n","      yules_q_statistic(e1,e2,y_test)\n","      for i,e1 in enumerate(ens_errsets)\n","      for e2 in ens_errsets[i+1:]])\n","\n","    # Compute the interrater agreement (See Eq. 16 of Kuncheva & Whitaker 2003)\n","    Dis_av = np.mean([\n","      disagreement_measure(e1,e2,y_test)\n","      for i,e1 in enumerate(ens_errsets)\n","      for e2 in ens_errsets[i+1:]])\n","    avg_acc = np.mean(accs)\n","    try:\n","      kappa = 1 - Dis_av / (2 * avg_acc * (1-avg_acc))\n","    except ZeroDivisionError:\n","      kappa = np.nan\n","\n","    # Compute the value of the LIT penalty\n","    gradcos2 = np.mean([elemwise_sq_cos_sim(g1, g2)\n","      for i,g1 in enumerate(grads)\n","      for g2 in grads[i+1:]])\n","\n","    print('  Q. statistic: {:.4f}'.format(q_stat))\n","    print('  Interrater agg: {:.4f}'.format(kappa))\n","    print('  Err. correl.: {:.4f}'.format(error_corr))\n","    print('  Av grad cos2: {:.4f}'.format(gradcos2))\n","\n","    row['q_stat'] = '{:.6f}'.format(q_stat)\n","    row['interrater'] = '{:.6f}'.format(kappa)\n","    row['err_corr'] = '{:.6f}'.format(error_corr)\n","    row['grad_cos2'] = '{:.6f}'.format(gradcos2)\n","  else:\n","    row['q_stat'] = 'nan'\n","    row['interrater'] = 'nan'\n","    row['err_corr'] = 'nan'\n","    row['grad_cos2'] = 'nan'\n","\n","  assert(set(row.keys()) == set(cols))\n","\n","  # Print everything to the CSV.\n","  write_row([row[k] for k in cols])\n","\n","# Define the neural network architecture - 256-unit hidden layer w/ dropout\n","if len(y_train.shape) == 1:\n","  y_shape = 2\n","else:\n","  y_shape = y_train.shape[1]\n","class Net(NeuralNetwork):\n","  @property\n","  def x_shape(self): return [None, X_train.shape[1]]\n","  @property\n","  def y_shape(self): return [None, y_shape]\n","  def rebuild_model(self, X, **_):\n","    L0 = X\n","    L1 = tf.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.relu)\n","    L1 = tf.layers.dropout(L1, training=self.is_train)\n","    L2 = tf.layers.dense(L1, y_shape, name=self.name+'/L2', activation=None)\n","    return [L1, L2]\n","\n","# Set up training parameters -- we'll use 0.0001 weight decay and train for the\n","# minimum epochs to run for 5000 iterations.\n","num_epochs = int(np.ceil(np.ceil((5000*128) / float(len(X_train)))))\n","train_args = [Net, n_models, X_train, y_train]\n","train_kwargs = {\n","  'num_epochs': num_epochs,\n","  'l2_weights': 0.0001,\n","  'print_every': 100\n","}\n","\n","# Train random restarts\n","tf.reset_default_graph()\n","save_models(train_restart_models(*train_args, **train_kwargs), 'restarts')\n","\n","# Train bagging\n","tf.reset_default_graph()\n","save_models(train_bagged_models(*train_args, **train_kwargs), 'baggings')\n","\n","# Train adaboost (using scikit-learn's default implementation)\n","tf.reset_default_graph()\n","adaboost = train_adaboost_models(*train_args, **train_kwargs)\n","adaboost_models = [e.mlp for e in adaboost.estimators_]\n","save_models(adaboost_models, 'adaboost',\n","        moe_auc=scoring_fun(adaboost.predict_proba(X_test), y_test),\n","        moe_val=scoring_fun(adaboost.predict_proba(X_val), y_val),\n","        moe_acc=accuracy_fun(adaboost.predict_proba(X_test), y_test))\n","\n","for penalty in np.logspace(-4, 1, 16):\n","  # Run LIT\n","  tf.reset_default_graph()\n","  save_models(\n","    train_diverse_models(*train_args, lambda_overlap=penalty, **train_kwargs),\n","    'diverse-{:.4f}'.format(penalty))\n","\n","  # Run NCL\n","  tf.reset_default_graph()\n","  save_models(\n","    train_neg_corr_models(*train_args, lambda_overlap=penalty, **train_kwargs),\n","    'negcorr-{:.4f}'.format(penalty))\n","\n","  # Run ACE\n","  tf.reset_default_graph()\n","  save_models(\n","    train_amended_xent_models(*train_args, lambda_overlap=penalty, **train_kwargs),\n","    'amended-{:.4f}'.format(penalty))"]},{"cell_type":"code","source":[""],"metadata":{"id":"iIuxjsy8slw_","executionInfo":{"status":"aborted","timestamp":1643207799090,"user_tz":-210,"elapsed":34,"user":{"displayName":"programmer el","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEEu2U7xuGGdRT1D_UDHmTp6p6SoRvJwRts74l=s64","userId":"06346042105450712556"}}},"execution_count":null,"outputs":[]}]}